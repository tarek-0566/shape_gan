{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysAmgMvHjrp_"
      },
      "source": [
        "\n",
        "# Lesson 8.0.0: Store this notebook! \n",
        "\n",
        "\n",
        "Go to \"File\" and make sure you store this file as a local copy to your Google Drive. If you do not have a Google account and also do not want to create one, please check Option *B* below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddv-6ADkjtQ-"
      },
      "source": [
        "Option A) Google Drive WITH collaboration\n",
        "\n",
        "If you want to work in a collaborative manner where each of you in the group can see each other's contributions, one of you needs to store the notebook in Google Drive and share it with the others. You share it by clicking on the SHARE button on the top right of this page and share the link with the \"everyone who receives this link can edit\" option with the other team members per e-mail, skype, or any other way you prefer.\n",
        "\n",
        "If you work with others, keep in mind to always copy the code before you edit it and always indicate your name as a comment (e.g. #Dagmar ) in the cell that it is clear who wrote which part. I also recommend creating a new code cell for your contributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK33HnCZjyot"
      },
      "source": [
        "Option B) Download this notebook as ipynb (Jupyter notebook) or py (Python file)\n",
        "\n",
        "To run either of these on your local machine requires the installation of the required programs, which for the first tutorial are Python and NLTK. This will become more as we continue on to machine learning (requiring sklearn) and deep learning (requiring tensorflow and/or pytorch). In Google Codelab all of these are provided and do not need to be installed locally.\n",
        "\n",
        "## License stuff\n",
        "This excercise is based on the Semantic Computing Course by [Dagmar Grohmann](https://github.com/dgromann/SemComp_WS2018).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llRJLqnF_4Hx"
      },
      "source": [
        "# Lesson 8.1: Pytorch tutorial - basics\n",
        "\n",
        "In order to get started with deep learning and practically code up neural networks, we need to familiarize ourselves with the packages that can be used to this end. There are two basic open source machine learning frameworks that can be used to this end: \n",
        "\n",
        "*   TensorFlow (Google)\n",
        "*   Torch (Facebook, Google DeepMind, Twitter)\n",
        "\n",
        "Since these are high level core libraries, it is easier to use a framework that builds on top of it and adds some usability and documentation. We are going to for once not use the Google solution, but will go with the Facebook solution of Pytorch. This first part of today's tutorial will introduce you to some core concepts of Pytorch before we start working with embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zHofgv7Ksp8",
        "outputId": "b3114013-b97b-4eb2-8392-b1353c2bc953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "# Let's first install pytorch \n",
        "!pip3 install torch torchvision tqdm gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT6LYQjvJ3pm"
      },
      "source": [
        "The most basic and important concept in Pytorch is that of a **Tensor**, To speed up computation and offer more flexibility, pytorch replaces numpy arrays with tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZtJy89bKGfk",
        "outputId": "5cde546b-182c-4811-e89e-b5cded9b4afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor: \n",
            "tensor([1., 1., 1., 1., 1.])\n",
            "Numpy array: \n",
            "[1. 1. 1. 1. 1.] \n",
            "\n",
            "Randomly initialized tensor:  tensor([[0.7576, 0.2793, 0.4031],\n",
            "        [0.7347, 0.0293, 0.7999],\n",
            "        [0.3971, 0.7544, 0.5695],\n",
            "        [0.4388, 0.6387, 0.5247],\n",
            "        [0.6826, 0.3051, 0.4635]]) \n",
            "\n",
            "Tensor initialized with zeros:  tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]]) \n",
            "\n",
            "Tensor initilialized with data:  tensor([5.5000, 3.0000]) \n",
            "\n",
            "Redefined randomly initialized tensor:  tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64) \n",
            "\n",
            "Initializing randomly based on the size of redefined:  tensor([[0.4550, 0.5725, 0.4980],\n",
            "        [0.9371, 0.6556, 0.3138],\n",
            "        [0.1980, 0.4162, 0.2843],\n",
            "        [0.3398, 0.5239, 0.7981],\n",
            "        [0.7718, 0.0112, 0.8100]]) \n",
            "\n",
            "torch.Size([5, 3]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy \n",
        "import tqdm\n",
        "\n",
        "# Seed for random number generator to ensure reproducibility of\n",
        "# random initializations\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#Difference between tensor and numpy array\n",
        "a = torch.ones(5)\n",
        "print(\"Tensor: \")\n",
        "print(a)\n",
        "print(\"Numpy array: \")\n",
        "print(a.numpy(), \"\\n\")\n",
        "\n",
        "\n",
        "# This creates a randomly initialized 5 x 3 matrix\n",
        "rand = torch.rand(5,3)\n",
        "print(\"Randomly initialized tensor: \", rand, \"\\n\")\n",
        "\n",
        "# This creates a 5 x 3 matriy filled with zeros and of dtype long\n",
        "# There are eight datatypes in tensor, this one is a datatype of 64-bit integer (signed)\n",
        "# Here are the others: https://pytorch.org/docs/stable/tensors.html\n",
        "zeros = torch.zeros(5,3, dtype=torch.long)\n",
        "print(\"Tensor initialized with zeros: \", zeros, \"\\n\")\n",
        "\n",
        "# Directly initialize a tensor with data \n",
        "data = torch.tensor([5.5, 3])\n",
        "print(\"Tensor initilialized with data: \", data, \"\\n\")\n",
        "\n",
        "# You can redefine an existing tensor \n",
        "redefined = rand.new_ones(5, 3, dtype=torch.double)\n",
        "print(\"Redefined randomly initialized tensor: \", redefined, \"\\n\")\n",
        "\n",
        "redefined_too = torch.rand_like(redefined, dtype=torch.float)\n",
        "print(\"Initializing randomly based on the size of redefined: \", redefined_too, \"\\n\")\n",
        "\n",
        "\n",
        "# Get the size - this is actually a tuple that supports tuple operations \n",
        "print(redefined_too.size(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFJM8p3JR3-v"
      },
      "source": [
        "Tensors in torch also support basic operations: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETVukvJ4R3P_",
        "outputId": "c8c32402-8dce-4859-beed-97aa81047ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition:  tensor([[1.3967, 1.2089, 1.4771],\n",
            "        [0.4001, 0.4698, 0.2781],\n",
            "        [1.2007, 1.1880, 1.0924],\n",
            "        [1.2657, 1.8914, 0.7650],\n",
            "        [1.0412, 1.3104, 0.8918]]) \n",
            "\n",
            "Addition alternative syntax:  tensor([[1.3967, 1.2089, 1.4771],\n",
            "        [0.4001, 0.4698, 0.2781],\n",
            "        [1.2007, 1.1880, 1.0924],\n",
            "        [1.2657, 1.8914, 0.7650],\n",
            "        [1.0412, 1.3104, 0.8918]]) \n",
            "\n",
            "Addition with tensor as argument:  tensor([[1.3967, 1.2089, 1.4771],\n",
            "        [0.4001, 0.4698, 0.2781],\n",
            "        [1.2007, 1.1880, 1.0924],\n",
            "        [1.2657, 1.8914, 0.7650],\n",
            "        [1.0412, 1.3104, 0.8918]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Addition of tensors matching in size\n",
        "x = torch.rand(5, 3)\n",
        "y= torch.rand(5, 3)\n",
        "print(\"Addition: \", x + y, \"\\n\")\n",
        "print(\"Addition alternative syntax: \", torch.add(x, y), \"\\n\")\n",
        "\n",
        "# Addition with providing a tensor as argument\n",
        "result = torch.empty(5, 3)\n",
        "print(\"Addition with tensor as argument: \", torch.add(x, y, out=result), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNEfziOWS3Wv"
      },
      "source": [
        "All opreations that mutate a tensor are indicated with an underscore _ such as the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l32jEjSBS2bk",
        "outputId": "9bbcbe7a-2e17-480f-bfda-edf50dac23e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding x to y:  tensor([[1.3967, 1.2089, 1.4771],\n",
            "        [0.4001, 0.4698, 0.2781],\n",
            "        [1.2007, 1.1880, 1.0924],\n",
            "        [1.2657, 1.8914, 0.7650],\n",
            "        [1.0412, 1.3104, 0.8918]])\n",
            "Adding 1:  tensor([[2.3967, 2.2089, 2.4771],\n",
            "        [1.4001, 1.4698, 1.2781],\n",
            "        [2.2007, 2.1880, 2.0924],\n",
            "        [2.2657, 2.8914, 1.7650],\n",
            "        [2.0412, 2.3104, 1.8918]])\n"
          ]
        }
      ],
      "source": [
        "# add x to y\n",
        "y.add_(x)\n",
        "print(\"Adding x to y: \", y)\n",
        "\n",
        "# you can also add a number \n",
        "print(\"Adding 1: \", y.add_(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v74kjaPZTINW"
      },
      "source": [
        "Indexing operations of tensors follow the numpy standard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7ZOTnMCTNDo",
        "outputId": "faf596fd-d5e1-488f-b403-81433816d771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:  tensor([[0.6397, 0.9743, 0.8300],\n",
            "        [0.0444, 0.0246, 0.2588],\n",
            "        [0.9391, 0.4167, 0.7140],\n",
            "        [0.2676, 0.9906, 0.2885],\n",
            "        [0.8750, 0.5059, 0.2366]]) \n",
            "\n",
            "Element at index one of each row of the matrix  tensor([0.9743, 0.0246, 0.4167, 0.9906, 0.5059]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Indexing\n",
        "print(\"X: \", x, \"\\n\")\n",
        "print(\"Element at index one of each row of the matrix \", x[:, 1], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qu1om-xTyoI"
      },
      "source": [
        "Resizing: if you wish to change the shape of the tensor you can use torch.view:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF_3EW5-UG1Z",
        "outputId": "6a3a2b9a-2862-4620-fca2-f10a3678d540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resizing: \n",
            "Original\n",
            "tensor([[ 0.4533,  1.1422,  0.2486, -1.7754],\n",
            "        [-0.0255, -1.0233, -0.5962, -1.0055],\n",
            "        [ 0.4285,  1.4761, -1.7869,  1.6103],\n",
            "        [-0.7040, -0.1853, -0.9962, -0.8313]])\n",
            "Resized view(16) tensor([ 0.4533,  1.1422,  0.2486, -1.7754, -0.0255, -1.0233, -0.5962, -1.0055,\n",
            "         0.4285,  1.4761, -1.7869,  1.6103, -0.7040, -0.1853, -0.9962, -0.8313]) \n",
            "\n",
            "Resized view(-1, 8) tensor([[ 0.4533,  1.1422,  0.2486, -1.7754, -0.0255, -1.0233, -0.5962, -1.0055],\n",
            "        [ 0.4285,  1.4761, -1.7869,  1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Exercise: resizing: what effect do the following\n",
        "# resize operations have on the tensors\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(\"Resizing: \")\n",
        "print(\"Original\")\n",
        "print(x)\n",
        "print(\"Resized view(16)\", y, \"\\n\")\n",
        "print(\"Resized view(-1, 8)\", z, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoMySrpVlUIJ"
      },
      "source": [
        "**Gradients and Backpropagation**\n",
        "\n",
        "If you set the flag  ```.requires_grad``` on a ```torch.Tensor``` to ```True``` the program will track all operations on it in order to enable later operations, such as backpropagation, which is very important to neural networks. \n",
        "\n",
        "When you finish all computations on your tensor, you can then simply call the function ```.backward()``` and have all the gradients computed automatically.. The gradient will then automatically be accumulated in the attribute ```.grad```. \n",
        "\n",
        "If you wish to disconnect a specific tensor from this process of tracking all operations, you can call the function ```.detach()```. This prevents future computations from being tracked. You can alternatively wrap the code block in a function ```with torch.no_grad()``` which does not track the operations on any variables included in the block. This is particularly helpful if you wish to evaluate a model that has trainable parameters with *required_grad=True* flags but for which we don't need the gradients in evaluation. \n",
        "\n",
        "There’s one more class which is very important for autograd implementation - a `Function`.\n",
        "\n",
        "`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the `Tensor` (except for Tensors created by the user - their grad_fn is None).\n",
        "\n",
        "If you want to compute the derivatives, you can call `.backward() `on a `Tensor`. If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYXem-00m_rw",
        "outputId": "54606965-b08b-4f99-eb6a-e88f10a670ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7f3fd59d92d0>\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# Tensor that requires gradien = operations are being tracked \n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# Let's do some operation \n",
        "y = x + 2 \n",
        "print(y)\n",
        "\n",
        "# y was created that has a grad_fn \n",
        "print(y.grad_fn)\n",
        "\n",
        "# Some more operations \n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)\n",
        "\n",
        "\n",
        "# Gradients\n",
        "# Let's calculate and print the gradietn (d(out)/dx) and print it\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "\n",
        "# Stop autograd from tracking \n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbyORAySgH4D"
      },
      "source": [
        "# Lesson 8.2:  Word Embeddings - first steps\n",
        "\n",
        "We will start looking at Pytorch and then play with existing embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bZEgkd6gYMC",
        "outputId": "463bcade-3109-4b97-dde6-a7b66c56d577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3ffc4ed610>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Again random number generator to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cODwB6TlGm1_"
      },
      "source": [
        "Here is a mini-example of how to initialize the layer randomly and with only two words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jvjHAr4sEdQ",
        "outputId": "3bf23303-1864-480a-83a4-e6566f296037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
            "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Map words to index to produce one-hot encodings \n",
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "\n",
        "# Initialize the embedding layer (nn = neural network) with the number of the \n",
        "# vocabulary and the dimensionality of the vectors \n",
        "# here: two words, vectors of 5 dimensions as ouput\n",
        "embeds = nn.Embedding(2, 5) \n",
        "lookup_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long)\n",
        "\n",
        "# Create a look up tensor for the random embeddings\n",
        "#for key, index in word_to_ix.items(): \n",
        "embeddings = embeds(lookup_tensor)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VviNW1RYouRq"
      },
      "source": [
        "Let's train our first embeddings. What do SGD and lr mean in the code below? What happens if you increase lr and \n",
        "increase the number of iterations?\n",
        "\n",
        "The below implementation is just a toy implementation. For a better version, see [this word2vec implementation in Pytorch](https://adoni.github.io/2017/11/08/word2vec-pytorch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxc6M8MctVJc"
      },
      "source": [
        "## 8.2.1 Simple Training language\n",
        "We start by defining our own simple training language. Our language only contains words like \"aaa\", \"bbb\", \"ccc\" , … \"xxx\", \"yyy\", \"zzz\". The sentences contain words, whose charachters are close in alphabet. In the resulting word embedding the vectors of \"aaa\" should be closest to \"bbb\" and furthest away from \"zzz\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhQkfLSSt3wW",
        "outputId": "c3d3bc1c-c699-40c1-e988-d3675ede0bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rrr sss ttt www xxx yyy xxx\n",
            "ddd ccc ddd ggg hhh ggg iii\n",
            "ooo nnn ooo ppp rrr sss ttt\n",
            "jjj kkk lll mmm lll nnn ooo\n",
            "nnn ppp qqq rrr sss ttt uuu\n",
            "jjj kkk lll mmm ooo ppp qqq\n",
            "iii kkk jjj mmm lll mmm ooo\n",
            "aaa bbb ddd fff ggg hhh ggg\n",
            "sss ttt uuu ttt www xxx yyy\n",
            "lll ooo ppp ooo rrr qqq sss\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from random import choice\n",
        "def generate_simple_language_corpus(SENTENCE_NUMBER=500, SENTENCE_LENGTH=7):\n",
        "  alpha_d = dict.fromkeys(string.ascii_lowercase, 0)\n",
        "\n",
        "  vocab_dataset = {}\n",
        "  for c, l in enumerate(alpha_d, 0):\n",
        "      vocab_dataset[c] = l*3\n",
        "\n",
        "  prev_word = None\n",
        "  simple_language_text =\"\"\n",
        "\n",
        "  for s in range(SENTENCE_NUMBER):\n",
        "      sentence = []\n",
        "      start_word = choice(range(0, 18))\n",
        "      for w in range(0, SENTENCE_LENGTH):\n",
        "          i = choice([x for x in range(start_word+w+0, start_word+w+3) if x not in [prev_word]])\n",
        "          sentence.append(vocab_dataset[i])\n",
        "          prev_word = i\n",
        "      simple_language_text += \" \".join(sentence) + \"\\n\"\n",
        "\n",
        "  return simple_language_text\n",
        "\n",
        "print(generate_simple_language_corpus(SENTENCE_NUMBER=10, SENTENCE_LENGTH=7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt-iRL_Iujfv"
      },
      "source": [
        "## 8.2.2 Train Word Embeddings from our simple language - naive version\n",
        "Now we are using a simple implementation to train Word Embeddings. Try to fill in the gaps \"______\" in the code below to solve this excercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncju5K1zow6h",
        "outputId": "197d578b-4c29-4597-d0a1-80ee2828bb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMBEDDING_DIM:  5\n",
            "SEN_NUM:  500\n",
            "EPOCHS:  200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [05:12<00:00,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10306.665680885315, 8795.842024445534, 7926.520627558231, 7374.788049757481, 6998.806492030621, 6720.31259137392, 6498.815441071987, 6314.494820743799, 6157.607008039951, 6021.004434674978, 5900.6817862689495, 5793.660476505756, 5697.22221532464, 5609.6908395290375, 5529.451138317585, 5455.260790318251, 5385.987034112215, 5320.99150377512, 5259.886064872146, 5202.21263371408, 5147.787080347538, 5096.057380720973, 5047.113083690405, 5000.70173612237, 4956.634393393993, 4914.830408543348, 4875.052357003093, 4837.006602317095, 4800.582947313786, 4765.855749800801, 4732.679603725672, 4700.910161778331, 4670.525296494365, 4641.351919069886, 4613.222907975316, 4586.0758541077375, 4559.799787521362, 4534.428732305765, 4509.874261647463, 4486.218099460006, 4463.388217791915, 4441.253765180707, 4419.738439396024, 4398.881995782256, 4378.731051206589, 4359.129873290658, 4340.081370353699, 4321.4973611831665, 4303.425154939294, 4285.932430967689, 4269.03170453012, 4252.461120530963, 4236.52613094449, 4221.050442457199, 4206.2115020900965, 4191.821134641767, 4177.866932094097, 4164.235751569271, 4151.048941284418, 4138.223678469658, 4125.633503466845, 4113.509649261832, 4101.739477440715, 4090.1434701532125, 4078.9725921303034, 4067.8788217157125, 4057.2507594823837, 4046.960094496608, 4036.7981367260218, 4026.8984550237656, 4017.2883243709803, 4007.9910855293274, 3998.6431854069233, 3989.6941794008017, 3980.9701109081507, 3972.458937935531, 3964.1529990956187, 3955.9253141209483, 3947.9799994826317, 3940.206620953977, 3932.5383765995502, 3925.0928277000785, 3917.8637282550335, 3910.6203746646643, 3903.588729918003, 3896.595217682421, 3889.8678547143936, 3883.2518131658435, 3876.732440263033, 3870.185804270208, 3864.093453787267, 3857.8785754665732, 3851.7945754677057, 3845.7475131452084, 3839.9073276519775, 3834.1291693747044, 3828.4115966260433, 3822.8041908219457, 3817.471366919577, 3812.0328260958195, 3806.670010790229, 3801.508449278772, 3796.342422477901, 3791.1253000721335, 3786.1440538093448, 3781.292140431702, 3776.4777230396867, 3771.707177862525, 3767.069684177637, 3762.5238479822874, 3758.039883032441, 3753.7165933176875, 3749.2451741993427, 3745.1372546628118, 3740.9352464973927, 3736.650318197906, 3732.541274473071, 3728.5366783067584, 3724.609128624201, 3720.6593933478, 3716.8646784201264, 3713.1547270715237, 3709.2354274094105, 3705.4434587582946, 3701.8735601231456, 3698.178017668426, 3694.5595403686166, 3691.029253013432, 3687.5397119894624, 3684.2783568128943, 3680.7744887545705, 3677.4959213510156, 3674.081565089524, 3670.849612094462, 3667.659124940634, 3664.6808738037944, 3661.5475790053606, 3658.2177400514483, 3655.2967911735177, 3652.2917251065373, 3649.32855065912, 3646.4574999436736, 3643.3787739649415, 3640.640693217516, 3637.7717475295067, 3634.995076291263, 3632.4321278855205, 3629.6192674413323, 3627.0984829589725, 3624.4596436470747, 3621.9660725817084, 3619.140707537532, 3616.647880680859, 3613.991095803678, 3611.5311249271035, 3609.005381897092, 3606.572695031762, 3604.203144364059, 3601.760359235108, 3599.383520513773, 3597.021277554333, 3594.7021549493074, 3592.523413516581, 3590.182228706777, 3587.902214191854, 3585.7101317495108, 3583.556414619088, 3581.3162186667323, 3578.9690823331475, 3577.018877416849, 3574.8373090326786, 3572.870267868042, 3570.8520285487175, 3568.6399152204394, 3566.713335312903, 3564.536189727485, 3562.6174149885774, 3560.5535081103444, 3558.792716830969, 3556.7268745675683, 3554.895628646016, 3552.968948557973, 3551.352319665253, 3549.1430240422487, 3547.587838932872, 3545.5786139443517, 3543.916537769139, 3541.9566224217415, 3540.50154671818, 3538.569954238832, 3536.9820196926594, 3535.147549316287, 3533.657629661262, 3531.6207276284695, 3530.167584106326, 3528.27559658885, 3526.697782382369, 3525.2131052315235, 3523.532475307584, 3521.9370557293296]\n",
            "eee: 1.0\n",
            "zzz: 0.8015705943107605\n",
            "bbb: 0.6195003986358643\n",
            "ggg: 0.6163588762283325\n",
            "ppp: 0.5735631585121155\n",
            "yyy: 0.5548444986343384\n",
            "mmm: 0.4702798128128052\n",
            "nnn: 0.39752325415611267\n",
            "hhh: 0.31925249099731445\n",
            "jjj: 0.0789942741394043\n",
            "www: 0.026498205959796906\n",
            "fff: 0.0013336545089259744\n",
            "rrr: -0.015946822240948677\n",
            "lll: -0.031236428767442703\n",
            "ooo: -0.12279357761144638\n",
            "ccc: -0.1648571789264679\n",
            "kkk: -0.17659969627857208\n",
            "ddd: -0.23111601173877716\n",
            "xxx: -0.3067185580730438\n",
            "iii: -0.3081722557544708\n",
            "sss: -0.38228797912597656\n",
            "vvv: -0.5657966732978821\n",
            "aaa: -0.5984352231025696\n",
            "uuu: -0.6231498718261719\n",
            "ttt: -0.6519352197647095\n",
            "qqq: -0.9266486167907715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_word_embedding_model(training_corpus, EMBEDDING_DIM = 10, EPOCHS = 300, CONTEXT_SIZE = 2):\n",
        "  test_sentence = training_corpus.split()\n",
        "\n",
        "\n",
        "  # we should tokenize the input, but we will ignore that for now\n",
        "  # build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "  trigrams = [([test_sentence[i-1], test_sentence[i + 1]], test_sentence[i ])\n",
        "              for i in range(1,len(test_sentence) - 1)]\n",
        "  # print the first 3, just so you can see what they look like\n",
        "  print(trigrams[:3])\n",
        "\n",
        "  # deduplicate \n",
        "  vocab = set(test_sentence)\n",
        "\n",
        "  # generate the word index\n",
        "  word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "  class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "      def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "          super(NGramLanguageModeler, self).__init__()\n",
        "          self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "          self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "          self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "      def forward(self, inputs):\n",
        "          embeds = self.embeddings(inputs).view((1, -1))\n",
        "          out = F.relu(self.linear1(embeds))\n",
        "          out = self.linear2(out)\n",
        "          log_probs = F.log_softmax(out, dim=1)\n",
        "          return log_probs\n",
        "\n",
        "\n",
        "  losses = []\n",
        "  loss_function = nn.NLLLoss()\n",
        "  model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "\n",
        "  # Exercise: What do SGD and lr mean? What happenes if you change them?\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(EPOCHS),total=EPOCHS):\n",
        "      total_loss = 0\n",
        "      for context, target in trigrams:\n",
        "\n",
        "          # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "          # into integer indices and wrap them in tensors)\n",
        "          context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "          # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "          # new instance, you need to zero out the gradients from the old\n",
        "          # instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 3. Run the forward pass, getting log probabilities over next\n",
        "          # words\n",
        "          log_probs = model(context_idxs)\n",
        "\n",
        "          # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "          # word wrapped in a tensor)\n",
        "          loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "          # Step 5. Do the backward pass and update the gradient\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      #print(\"\\t\", total_loss)\n",
        "      losses.append(total_loss)\n",
        "  print(losses) # The loss decreased every iteration over the training data!\n",
        "\n",
        "  return model, word_to_ix, losses\n",
        "training_corpus = generate_simple_language_corpus(SENTENCE_NUMBER=500, SENTENCE_LENGTH=7)\n",
        "model, word_to_ix, losses = train_word_embedding_model(training_corpus, EMBEDDING_DIM=5, EPOCHS=10, CONTEXT_SIZE = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9kw_-OZvvmg"
      },
      "source": [
        "# 8.2.3 Work with our Word-Embeddings\n",
        "Now we are going to analyse how good our word embeddings were. For that we define first a function returning us the word embedding for a given word, and then a function sorting all word embeddings after their similarity to our target word.\n",
        "\n",
        "Can you tweak the parameters from the word-embedding generation function to produce better results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOZpy0FivKTB",
        "outputId": "efa01fc4-8c69-47c0-d4c5-438ffc01dc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eee: 1.0\n",
            "hhh: 0.408682644367218\n",
            "aaa: 0.33671995997428894\n",
            "ppp: 0.15390297770500183\n",
            "xxx: 0.12604454159736633\n",
            "zzz: 0.10617038607597351\n",
            "nnn: 0.06814315170049667\n",
            "uuu: 0.038116198033094406\n",
            "fff: 0.031432054936885834\n",
            "mmm: -0.019169578328728676\n",
            "jjj: -0.06592382490634918\n",
            "vvv: -0.1929464191198349\n",
            "rrr: -0.29188206791877747\n",
            "yyy: -0.31268852949142456\n",
            "kkk: -0.32156622409820557\n",
            "www: -0.4094087481498718\n",
            "ccc: -0.41429662704467773\n",
            "iii: -0.4157615900039673\n",
            "ttt: -0.42468011379241943\n",
            "lll: -0.45957136154174805\n",
            "qqq: -0.48173922300338745\n",
            "ddd: -0.5246429443359375\n",
            "ggg: -0.5451634526252747\n",
            "ooo: -0.5874466300010681\n",
            "bbb: -0.6789381504058838\n",
            "sss: -0.8591558933258057\n"
          ]
        }
      ],
      "source": [
        "def get_word_embedding_for_word(word_to_test, word_to_ix):\n",
        "  word_to_test_ix = word_to_ix[word_to_test]\n",
        "  word_to_test_torch = torch.tensor([word_to_test_ix], dtype=torch.long)\n",
        "  \n",
        "  embedding_of_word_to_test = model.embeddings(word_to_test_torch)\n",
        "  return embedding_of_word_to_test\n",
        "\n",
        "def most_similar(word_to_test, word_to_ix):\n",
        "  test_embedding = get_word_embedding_for_word(word_to_test, word_to_ix)\n",
        "\n",
        "  # get embeddings for all other possible words like aaa bbb ccc\n",
        "  cos = torch.nn.CosineSimilarity()\n",
        "  results = {}\n",
        "  for c in string.ascii_lowercase:\n",
        "    c_embedding = get_word_embedding_for_word(c+c+c, word_to_ix)\n",
        "\n",
        "    cosine_similarity = cos(test_embedding, c_embedding)\n",
        "    results[c+c+c] = cosine_similarity.item()\n",
        "  sorted_results =  dict(sorted(results.items(), key=lambda item: -item[1]))\n",
        "  return sorted_results\n",
        "\n",
        "sims = most_similar(\"eee\", word_to_ix)\n",
        "\n",
        "for k,v in sims.items():\n",
        "  print(\"{}: {}\".format(k,v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoIZYG5vwxz6"
      },
      "source": [
        "## 8.2.4 Better Implementation\n",
        "We are using now our generated simple language and a much more optimized implementation of word-embeddings, the ones in the library [gensim](https://radimrehurek.com/gensim/).\n",
        "\n",
        "Again we are displaying the most similar vectors. Can you notice a difference in performance to our naive implementation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJfMwFa4qinx",
        "outputId": "db558783-0698-4128-f40a-614993aaaeba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ddd', 0.9979580640792847),\n",
              " ('bbb', 0.993108868598938),\n",
              " ('eee', 0.9924113750457764),\n",
              " ('jjj', 0.9917413592338562),\n",
              " ('kkk', 0.991080105304718),\n",
              " ('iii', 0.9904963970184326),\n",
              " ('ggg', 0.9902485013008118),\n",
              " ('hhh', 0.9890344738960266),\n",
              " ('fff', 0.9880383014678955),\n",
              " ('aaa', 0.9762473702430725)]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim \n",
        "from sklearn.decomposition import PCA \n",
        "from matplotlib import pyplot \n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "training_corpus =  generate_simple_language_corpus(SENTENCE_NUMBER=3000, SENTENCE_LENGTH=7)\n",
        "\n",
        "# we have to convert the corpus to a different form for gensim\n",
        "training_corpus_gensim = [w.split(\" \") for w in training_corpus.split(\"\\n\")]\n",
        "\n",
        "\n",
        "model = gensim.models.Word2Vec(training_corpus_gensim, window=2) \n",
        "\n",
        "model.most_similar('ccc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW3IPEwgLCrO"
      },
      "source": [
        "# Lesson 8.3: Use existing embeddings\n",
        "\n",
        "Now we are using already pre-trained word embeddings.\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMaN6E36JzNI",
        "outputId": "05f780e4-b7ba-46cf-a72a-d7d6781739a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-01-11 11:01:21--  https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial6/word2vec_embeddings.bin [following]\n",
            "--2022-01-11 11:01:21--  https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial6/word2vec_embeddings.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96769269 (92M) [application/octet-stream]\n",
            "Saving to: ‘word2vec_embeddings.bin.1’\n",
            "\n",
            "word2vec_embeddings 100%[===================>]  92.29M   219MB/s    in 0.4s    \n",
            "\n",
            "2022-01-11 11:01:22 (219 MB/s) - ‘word2vec_embeddings.bin.1’ saved [96769269/96769269]\n",
            "\n",
            "--2022-01-11 11:01:22--  https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial6/analogy.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272272 (266K) [text/plain]\n",
            "Saving to: ‘analogy.txt.1’\n",
            "\n",
            "analogy.txt.1       100%[===================>] 265.89K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-01-11 11:01:22 (11.3 MB/s) - ‘analogy.txt.1’ saved [272272/272272]\n",
            "\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a \n",
        "# large corpus of news documents  \n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
        "!wget https://raw.githubusercontent.com/dgromann/SemComp_WS2018/master/Tutorial6/analogy.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZG8KQMILXz-"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69aLE4-7LxlO",
        "outputId": "d2993784-6cd6-4c34-cf15-6d8702bba5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80000\n",
            "[ 0.04052734  0.0625     -0.01745605  0.07861328  0.03271484 -0.01263428\n",
            "  0.00964355  0.12353516 -0.02148438  0.15234375 -0.05834961 -0.10644531\n",
            "  0.02124023  0.13574219 -0.13183594  0.17675781  0.27148438  0.13769531\n",
            " -0.17382812 -0.14160156 -0.03076172  0.19628906 -0.03295898  0.125\n",
            "  0.25390625  0.12695312 -0.15234375  0.03198242  0.01135254 -0.01361084\n",
            " -0.12890625  0.01019287  0.23925781 -0.08447266  0.140625    0.13085938\n",
            " -0.04516602  0.06494141  0.02539062  0.05615234  0.24609375 -0.20507812\n",
            "  0.23632812 -0.00860596 -0.02294922  0.05078125  0.10644531 -0.03564453\n",
            "  0.08740234 -0.05712891  0.08496094  0.23535156 -0.10107422 -0.03564453\n",
            " -0.04736328  0.04736328 -0.14550781 -0.10986328  0.14746094 -0.23242188\n",
            " -0.07275391  0.19628906 -0.37890625 -0.07226562  0.04833984  0.11914062\n",
            "  0.06103516 -0.12109375 -0.27929688  0.05200195  0.04907227 -0.02709961\n",
            "  0.1328125   0.03369141 -0.32226562  0.04223633 -0.08789062  0.15429688\n",
            "  0.09472656  0.10351562 -0.02856445  0.00128174 -0.00427246  0.24609375\n",
            " -0.05957031 -0.16894531 -0.09619141  0.16796875  0.0133667   0.04882812\n",
            "  0.08349609  0.06347656 -0.00872803 -0.08642578 -0.03857422 -0.08251953\n",
            "  0.15722656  0.22753906 -0.00762939 -0.19921875 -0.06347656  0.12792969\n",
            " -0.06347656 -0.03027344  0.0456543   0.06298828 -0.02526855 -0.06787109\n",
            " -0.01141357 -0.13574219  0.02978516  0.10400391 -0.15917969 -0.08447266\n",
            "  0.29882812 -0.12597656  0.11425781 -0.08105469 -0.09082031 -0.07910156\n",
            " -0.11181641 -0.09619141  0.02770996  0.14257812 -0.26757812 -0.09375\n",
            "  0.03979492 -0.17871094 -0.02819824  0.01464844 -0.31640625 -0.24511719\n",
            " -0.08935547  0.09716797 -0.00964355 -0.14746094  0.15234375  0.21582031\n",
            "  0.05981445  0.23828125 -0.05151367  0.14941406  0.13574219 -0.03222656\n",
            " -0.265625   -0.11181641 -0.23046875 -0.140625    0.25585938 -0.15429688\n",
            "  0.1796875   0.15527344 -0.21582031  0.36328125 -0.1015625   0.04980469\n",
            "  0.07177734 -0.14550781 -0.03198242  0.00952148 -0.12109375  0.12109375\n",
            "  0.09765625  0.07763672  0.3203125  -0.22265625 -0.08447266 -0.10742188\n",
            "  0.11279297 -0.13867188 -0.21875     0.0145874   0.13378906 -0.00921631\n",
            "  0.00921631  0.16894531  0.16894531 -0.078125   -0.00665283  0.03735352\n",
            " -0.10888672 -0.25390625  0.01452637 -0.09716797 -0.19628906 -0.01782227\n",
            " -0.28125    -0.02050781 -0.02905273 -0.09375    -0.17675781  0.21484375\n",
            " -0.05224609 -0.11572266 -0.01977539 -0.10839844 -0.01342773 -0.15332031\n",
            " -0.140625   -0.11816406  0.09228516  0.109375    0.05761719 -0.03466797\n",
            "  0.03564453 -0.12011719 -0.14257812 -0.00072479 -0.06689453  0.11914062\n",
            " -0.10449219  0.07861328 -0.12792969  0.09570312 -0.00817871  0.07128906\n",
            "  0.20703125 -0.03149414  0.09570312  0.17285156 -0.07958984 -0.02429199\n",
            " -0.07519531 -0.07568359  0.09521484 -0.06494141 -0.00689697 -0.09033203\n",
            "  0.03100586  0.19921875 -0.10644531 -0.11474609  0.18652344 -0.05078125\n",
            "  0.0859375   0.00128937 -0.18847656 -0.20019531 -0.02832031  0.11328125\n",
            "  0.25976562  0.22070312  0.04101562  0.00171661  0.07568359 -0.01196289\n",
            "  0.0177002  -0.05883789 -0.25976562 -0.234375   -0.04956055  0.25976562\n",
            "  0.15332031  0.15136719  0.08300781 -0.15527344  0.04931641  0.07519531\n",
            " -0.05078125 -0.1328125  -0.13574219  0.04199219 -0.14257812  0.02099609\n",
            "  0.07861328  0.01611328  0.01623535 -0.21582031  0.01599121 -0.04882812\n",
            " -0.02404785  0.13476562  0.08496094 -0.01196289  0.10009766 -0.13867188\n",
            "  0.08056641 -0.22070312 -0.12011719  0.18945312  0.05444336 -0.05053711\n",
            "  0.00147247  0.14160156 -0.06494141 -0.05566406 -0.09033203 -0.0267334\n",
            " -0.10498047  0.02416992  0.01422119  0.1875     -0.16503906  0.01538086\n",
            " -0.04174805  0.05444336 -0.01184082 -0.15625     0.00193024 -0.06982422]\n",
            "[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348937988281), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728254318237), ('solid', 0.5806034803390503), ('lousy', 0.5764201879501343)]\n",
            "[('queen', 0.4827326238155365)]\n"
          ]
        }
      ],
      "source": [
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin\", binary=True)\n",
        "\n",
        "# Print the length fo the whole vocabulary \n",
        "print(len(model.wv.vocab))\n",
        "\n",
        "# Print the embedding of a specific word \n",
        "print(model[\"good\"])\n",
        "\n",
        "# Get the 10 most similar words of \"good\"\n",
        "print(model.most_similar(\"good\", topn=10))\n",
        "\n",
        "# Check whether our embeddings are good at the analogy task\n",
        "print(model.most_similar(positive=['women', 'king'], negative=['man'], topn=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SDE_Exercise_8_Word_Embeddings_SOLUTIONS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}